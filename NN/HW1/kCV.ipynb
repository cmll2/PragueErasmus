{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e999f046",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6c3ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Camille HascoÃ«t\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f4078",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "083ac7e4-6d64-4e19-91d2-47755c989758",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "204573db62c035eb0c64319f10a5c92d",
     "grade": false,
     "grade_id": "cell-c21816ed2c90482a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of Jupyter is too old, please update it.\"\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from typing import List, Set, Dict, Tuple\n",
    "from numpy.testing import assert_approx_equal, assert_allclose\n",
    "rng = default_rng(2023) # common random number generator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs, make_classification, make_moons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3e5c23-c691-419e-bc56-7845830649be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11aab67cdb3a7fc167c8ace70ecc5b53",
     "grade": false,
     "grade_id": "cell-a6c9a36ba24a23d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Assignment: $k$-Fold Cross-Validation\n",
    "\n",
    "In this assignment, you will implement a function that compares two learning algorithms using $k$-fold cross-validation and apply it to compare some simple learning algorithms.\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement class `Perceptron` for the perceptron learning algorithm. **(2 points)**\n",
    "2. Implement function `cross_val` for evaluating the difference of errors between two given learning algorithms using $k$-fold cross-validation. **(4 points)**\n",
    "3. Implement function `conf_interval` for computing confidence interval for an error difference when using $k$-fold cross-validation. **(1 point)**\n",
    "4. By applying the functions implemented in the above tasks, compare several learning algorithms. **(3 points)**\n",
    "\n",
    "This notebook was generated using `nbgrader`. It has a special format. Several cells contain tests. Some of the tests are hidden. Please, **do not break the format**:\n",
    "1. do not delete cells where you should insert your answers (code or text), and\n",
    "2. do not copy complete cells (you can freely copy the contents of cells).\n",
    "Otherwise, you can add and delete cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e515081c-ea02-46b1-bd4b-2212248a27cd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8310eb164d92058abf6930ac41fc714d",
     "grade": false,
     "grade_id": "cell-a6c9a36ba24a23d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A learning algorithm tries to learn a target function $f: \\mathrm{R}^n \\to \\{0,1\\}$, where $\\mathrm{R}$ is the set of real numbers. The goal of a learning algorithm is to identify a function $h: \\mathrm{R}^n \\to \\{0,1\\}$, called *hypothesis*, from some class of functions $\\mathcal{H}$ such that the function $h$ is a good approximation of the target function $f$. The only information the algorithm can use is a sample $S\\subset X$ called a *training set*, and the correct value $f(x)$, called *label*, for all $x \\in S$. The sample $S$ is a set of $n$ elements from $X$ randomly selected according to some probabilistic distribution $D$.\n",
    "\n",
    "We suppose that each learning algorithm is implemented as a subclass of the following \n",
    "class `LearningAlgorithm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4298b21f-53c2-4a1d-b19e-e1ed805b4b98",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "375ecc5f490401cb38489844c1b3f743",
     "grade": false,
     "grade_id": "cell-LearningAlgorithm",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LearningAlgorithm:\n",
    "    \"\"\" Base class for learning algorithms. \"\"\"\n",
    "\n",
    "    def __init__(self, **learning_par):\n",
    "        self.par = learning_par\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\" Learn the parameters of the algorithm. \n",
    "        The learned parameters are stored into member variables.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, X: np.ndarray) -> np.array:\n",
    "        \"\"\" Apply the learned function to the input vectors `X`. \n",
    "        The returned value is a vector of the results - zeros and ones.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def score(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\" Compute the mean accuracy on the test vectors `X`, where `y` contains the \n",
    "        correct labels for the vectors (the rows) in `X`. \n",
    "        \"\"\"\n",
    "        output = self.predict(X)\n",
    "        return np.mean(output == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb60ff4-9bdc-4687-9f8b-09905cf90893",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4662870641c9b1850999282fadecf826",
     "grade": false,
     "grade_id": "cell-a6c9a36ba24a23d4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "where\n",
    "\n",
    "Param      |Meaning\n",
    "--------------|---------------------------------------------------------------\n",
    "`learning_par`|is a dictionary of parameters of the learning algorithm,\n",
    "`fit`         |is the learning function; it computes learned parameters and stores them into member variables; it can use the parameters from the member variable `par`,\n",
    "`X`           |is a two-dimensional numpy array (training and test vectors are the rows of `X`),\n",
    "`y`           |is a vector of desired labels (zeros and ones), and\n",
    "`predict`     |computes the learned function for the input vectors from a two-dimensional numpy array `X` (each row of the array is an input vector); it uses the learned parameters and/or the parameters from the member variable `par`; the returned value is a vector of the results - zeros and ones,\n",
    "`score`       |computes the mean accuracy of the trained algorithm on the test vectors X, where y contains the correct labels for the vectors (the rows) in X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c4513-9495-4a6b-b2fb-d8823478cd46",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1dc85cdbc497060773a64cf62f635c0c",
     "grade": false,
     "grade_id": "cell-e58f901685458e12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## A simple learning algorithm `Memorizer`\n",
    "\n",
    "Below, we implement a simple learning algorithm called `Memorizer` that memorizes all training samples and their true labels. Trained `Memorizer` answers \n",
    "* correctly on the inputs from the training set, and \n",
    "* randomly otherwise - it outputs zeroes and ones with the same ratio as in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc4b0e0f-99d8-456f-b377-bfadb9ab5844",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f37e0e2b57acd51704040e974def95fe",
     "grade": false,
     "grade_id": "cell-Memorizer",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Memorizer(LearningAlgorithm):\n",
    "    \n",
    "    def __init__(self, rng_seed=42):\n",
    "        super().__init__(_seed=rng_seed, _rng=default_rng(rng_seed))\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        self._learned_par = [np.atleast_2d(X), y]\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.array:\n",
    "        # generate random outputs\n",
    "        # the following line of code ensures that each call of predict on the same `X` \n",
    "        # will return the same output; for a \"serious\" application, this line should be omitted\n",
    "        self.par['_rng'] = default_rng(self.par['_seed'])\n",
    "        \n",
    "        X = np.atleast_2d(X)\n",
    "        out = self.par['_rng'].binomial(1, self._learned_par[1].mean(), X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            res = (self._learned_par[0] == X[i]).all(axis=1).nonzero()[0]\n",
    "            if res.size > 0:\n",
    "                out[i] = self._learned_par[1][res[-1]]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986b8247-7d18-4992-9e3b-eff867ff5f2b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98ac0a88c67a40a0c50b154edc682ae8",
     "grade": false,
     "grade_id": "cell-0d960bb39b55b2a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "The above class can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "502fbf2c-b33a-46f2-9df1-e231bef8df1f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fef2a53b0de0f7749e75d3f09ee0d073",
     "grade": false,
     "grade_id": "cell-a1a49b7a8f5100b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for input list [0.0, 1.0] is 1\n",
      "The prediction for input array [0. 0.] is 0\n",
      "par: {'_seed': 42, '_rng': Generator(PCG64) at 0x16F101951C0}\n",
      "prediction: [0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 0] positive predictions: 10\n",
      "score: 0.64\n",
      "par: {'_seed': 2023, '_rng': Generator(PCG64) at 0x16F101952A0}\n",
      "prediction: [0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1] positive predictions: 14\n",
      "score: 0.48\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0], [0.0, 0.0]])\n",
    "y_train = np.array([1, 1, 0, 1, 0])\n",
    "\n",
    "X_test = np.vstack((X_train, np.arange(0.1, 4.01, 0.1).reshape(-1, 2)))\n",
    "y_test = np.hstack((y_train, np.zeros(X_test.shape[0] - y_train.shape[0])))\n",
    "\n",
    "m = Memorizer()\n",
    "m.fit(X_train, y_train)\n",
    "\n",
    "# trained memorizer can predict the label for a sample represented as a list\n",
    "X_list = [0.0, 1.0]\n",
    "print(f'The prediction for input list {X_list} is {m.predict(X_list)[0]}')\n",
    "\n",
    "# trained memorizer can predict the label for a sample represented as a numpy array\n",
    "X_array = np.array([0.0, 0.0])\n",
    "print(f'The prediction for input array {X_array} is {m.predict(X_array)[0]}')\n",
    "\n",
    "prediction = m.predict(X_test)\n",
    "print(f'par: {m.par}\\nprediction: {prediction} positive predictions: {prediction.sum()}' \n",
    "      f'\\nscore: {m.score(X_test, y_test)}')\n",
    "\n",
    "# now we change the seed for the random number generator\n",
    "m = Memorizer(rng_seed=2023)\n",
    "m.fit(X_train, y_train)\n",
    "\n",
    "prediction = m.predict(X_test)\n",
    "print(f'par: {m.par}\\nprediction: {prediction} positive predictions: {prediction.sum()}'\n",
    "      f'\\nscore: {m.score(X_test, y_test)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c036fd9-e05c-4b6e-ad14-5c3475d3a4e3",
   "metadata": {},
   "source": [
    "## Task 1: Implement perceptron (2 points)\n",
    "\n",
    "Similarly, we can adapt our implementation of perceptron learning algorithm from a previous lab.\n",
    "Complete the implementation below. The extended weight vector will be initialized as \n",
    "`np.asarray(init_weights, dtype=float)` - this enables that `init_weights` \n",
    "can be either list of floats, or numpy array, and the empty weigth vector \n",
    "can be tested using `self.par['weights'].size == 0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e2c7be6-f528-40c2-8803-b3f57ad13acf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca7ae2ff8b3986bb15aa36781cf7d993",
     "grade": false,
     "grade_id": "cell-Perceptron",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Perceptron(LearningAlgorithm):\n",
    "    \n",
    "    def __init__(self, init_weights=[], lr=1.0, max_epochs=1000):\n",
    "        # Perceptron constructor\n",
    "        # `init_weights` are the initial weights (including the bias) of the perceptron\n",
    "        # `lr` is the learning rate\n",
    "        # `max_epochs` is the maximum number of epochs\n",
    "        super().__init__(weights = np.asarray(init_weights, dtype=float), lr=lr, max_epochs=max_epochs)\n",
    "\n",
    "    def set_bias(self, X):\n",
    "        X = np.asarray(X)\n",
    "        if len(X.shape) == 1:\n",
    "            return np.append(X, 1)\n",
    "        else:\n",
    "            return np.c_[X, np.ones(X.shape[0])]\n",
    "        \n",
    "    def predict(self, X: np.ndarray) -> np.array:\n",
    "        # Compute the output of the perceptron.\n",
    "        # Input `X` can be\n",
    "        #  * a vector, i.e., one sample\n",
    "        #  * or a two-dimensional array, where each row is a sample.\n",
    "        # Returns\n",
    "        #  * a vector with values 0/1 with the output of the perceptron \n",
    "        #    for all samples in `X`\n",
    "        # Raises an exception if the weights are not initialized.\n",
    "        X = self.set_bias(X)\n",
    "        if not np.any(self.par['weights']):\n",
    "            raise Exception('The weights are not initialized.')\n",
    "        if len(X.shape) == 1:\n",
    "            return [1] if np.dot(self.par['weights'], X) >= 0 else [0]\n",
    "        else:\n",
    "            return np.array([1 if np.dot(self.par['weights'], x) >= 0 else 0 for x in X])\n",
    "    \n",
    "    def partial_fit(self, X: np.ndarray, y: np.ndarray, lr=1.0) -> None:\n",
    "        # perform one epoch perceptron learning algorithm \n",
    "        # on the training set `X` (two-dimensional numpy array of floats) with \n",
    "        # the desired outputs `y` (vector of integers 0/1) and learning rate `lr`.\n",
    "        # If self.par['weights'] is empty, the weight vector is generated randomly.\n",
    "        if not np.any(self.par['weights']):\n",
    "            self.par['weights'] = np.random.rand(X.shape[1] + 1)\n",
    "        for x, target in zip(X, y):\n",
    "            x_bias = np.append(x, 1)\n",
    "            self.par['weights'] += x_bias * lr * (target - self.predict(x))\n",
    "            \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, lr=None, max_epochs=None) -> int:\n",
    "        # trains perceptron using perceptron learning algorithm\n",
    "        # on the training set `X` (two-dimensional numpy array of floats) with \n",
    "        # the desired outputs `y` (vector of integers 0/1). \n",
    "        # If `self.par['weights'] is empty, the weight vector is generated randomly.\n",
    "        # If the learning rate `lr` is `None`, `self.par['lr']` is used.\n",
    "        # If `max_epochs` is `None`, `self.par['max_epochs']` is used. \n",
    "        # Returns the number of epochs used in the training (at most `max_epochs`).\n",
    "        if max_epochs == None:\n",
    "            max_epochs = self.par['max_epochs']\n",
    "        if lr == None:\n",
    "            lr = self.par['lr']\n",
    "        for epochs in range (max_epochs):\n",
    "            #print(self.par['weights'])\n",
    "            self.partial_fit(X, y, lr)\n",
    "            \n",
    "            if self.score(X, y) == 1:\n",
    "                print(self.par['weights'])\n",
    "                return epochs + 1\n",
    "        return max_epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc429443-1a1e-4aa1-8d9d-b0595b8eabb4",
   "metadata": {},
   "source": [
    "A perceptron without weights cannot make predictions and must throw an exception (**0.5 points for this part**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f431f11-9e6c-48ac-825f-ac211f0252f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10a43f11dbf1ef906908e02138c5b59f",
     "grade": true,
     "grade_id": "cell-9949134961679c10",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = Perceptron(max_epochs=10)\n",
    "try:\n",
    "    p.predict([1,1])\n",
    "except Exception as e:\n",
    "    pass\n",
    "else:\n",
    "    raise AssertionError(\"Perceptron.predict with empty weights did not raise an exception\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72509b03-6917-47d5-84eb-15f5e8f558cb",
   "metadata": {},
   "source": [
    "Test the implementation of `Perceptron` carefully. It should pass all the tests below and some additional hidden tests (**1.5 points for this part**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1add996-caf6-4ded-986f-f15ad892fe19",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "489e5f9cb15a26161728e79ac1166033",
     "grade": true,
     "grade_id": "cell-e854c2188c083a22",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the input list [0.0, 1.0] is 0\n",
      "Prediction for the input array [0. 0.] is 1\n",
      "[ 0. -1.  0.]\n",
      "Training required 2 epoch(s)\n",
      "Trained perceptron {'weights': array([ 0., -1.,  0.]), 'lr': 1.0, 'max_epochs': 10}\n",
      "Score on the training set 1.0\n"
     ]
    }
   ],
   "source": [
    "p = Perceptron(init_weights=[1,-2,1], max_epochs=10)\n",
    "assert (p.predict(np.array([[6,3],[5,3],[1,1],[1,1.00001]])) == [1,1,1,0]).all()\n",
    "\n",
    "X_list = [0.0, 1.0]\n",
    "print(f\"Prediction for the input list {X_list} is {p.predict(X_list)[0]}\")\n",
    "assert (p.predict(X_list)[0] == 0)\n",
    "\n",
    "X_array = np.array([0.0, 0.0])\n",
    "print(f\"Prediction for the input array {X_array} is {p.predict(X_array)[0]}\")\n",
    "assert (p.predict(X_array)[0] == 1)\n",
    "\n",
    "p = Perceptron(init_weights=[1,1,1], max_epochs=10)\n",
    "X_train = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "y_train = np.array([ 1, 0, 1, 0])\n",
    "\n",
    "epochs = p.fit(X_train, y_train)\n",
    "print(f\"Training required {epochs} epoch(s)\")\n",
    "assert epochs == 2\n",
    "print(f\"Trained perceptron {p.par}\")\n",
    "#assert (p.par['weights'] == np.array([ 0., -1.,  0.])).all()\n",
    "print(f\"Score on the training set {p.score(X_train, y_train)}\")\n",
    "assert_approx_equal(p.score(X_train, y_train), 1.0)\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, random_state=1234)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n",
    "p = Perceptron([-1,1,2],0.5,5)\n",
    "epochs = p.fit(X_train, y_train)\n",
    "assert epochs==5\n",
    "assert_approx_equal(p.score(X_test, y_test), 0.86666666)\n",
    "assert_allclose(p.par['weights'], [0.04921529, 1.30631335, 0.        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55fad99-d7c7-4e6a-8939-c480a7324641",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac5f470b5893a8896ae93e3cf95dc04b",
     "grade": false,
     "grade_id": "cell-2af7953d9fc3d009",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2: Implement k-fold cross-validation (4 points)\n",
    "\n",
    "Then it is easy to implement the following function `cross_val` that estimates the difference between the error of the hypothesis\n",
    "learned by a learning algorithm `learn_alg1` and the error of the hypotheses learned by a learning algorithm\n",
    "`learn_alg2` using `k`-fold cross-validation on\n",
    "patterns `X` with the desired outputs `y`. The function returns the estimated\n",
    "difference of errors `delta` and estimated standard deviation `s` of this\n",
    "estimator. **You should implement your own function using `numpy`, not use \n",
    "any implementation of cross-validation from any third-party library!**\n",
    "\n",
    "* If the value of `shuffle` is `False`, then\n",
    "  * the order of samples must not be changed before partitioning into folds for `k`-fold cross-validation, \n",
    "  * all folds should be continuous parts of `X`, and\n",
    "  * the first fold (from the beginning of array `X`) must be used as the first test set and so on until the last fold is used as the last test set.\n",
    "* If the value of `shuffle` is `True`, then \n",
    "  * the patterns from `X` should be assigned randomly into `k` folds (then, in general, calling `cross_val` repeatedly with the same parameters can result in different folds and different outputs).\n",
    "\n",
    "_Notes:_ \n",
    "* _The sizes of the folds can differ by at most 1._\n",
    "* _The function computes **estimates** of the error difference and standard deviation of the estimated difference of errors. Therefore, it should compute the **sample** standard deviation according to the formula $$\\sigma = \\sqrt{\\frac{1}{k-1}\\sum_{i=1}^k (\\delta_i - \\bar{\\delta})^2}$$ where $\\delta_i$ is the error difference if the $i$-th fold is used as the test set, and $\\bar{\\delta}$ is the average error difference on all folds._\n",
    "* _Be aware that for each iteration of k-fold cross-validation, the learning algorithms must start from the same state of the learning algorithm. E.g., you can make a copy of a `learn_alg1` using `copy.deepcopy(learn_alg1)`._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "263764f1-8ede-4b01-8770-58081f913716",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edeadfa9fb76d478b3b1945e396727de",
     "grade": false,
     "grade_id": "cell-cross_val",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def cross_val(k: int, learn_alg1: LearningAlgorithm, learn_alg2: LearningAlgorithm, \n",
    "              X: np.ndarray, y: np.ndarray, shuffle: bool = True, verbose=0) -> Tuple[float, float]:\n",
    "    '''Estimates the difference between errors and the standard deviation of the difference for\n",
    "    two learning algorithms.\n",
    "    \n",
    "        delta, std = cross_val(k, learn_alg1, learn_alg2, X, y, shuffle, verbose)\n",
    "                               \n",
    "    Args:\n",
    "        k:           The number of folds used in k-fold cross-validation.\n",
    "        learn_alg1:  The first learning algorithm.\n",
    "        learn_alg2:  The second learning algorithm.\n",
    "        name_apply1: The name of the function for applying the first learned function.\n",
    "        X:           (2-d numpy array of floats): The training set; samples are rows.\n",
    "        y:           (vector of integers 0/1): The desired outputs for the training samples.\n",
    "        shuffle: If True, shuffle the samples into folds; otherwise, do not shuffle\n",
    "                 the samples before splitting them into folds.\n",
    "        verbose: If verbose == 0, it prints nothing. \n",
    "                 If verbose == 1, prints errors of both algorithms for each fold as a tuple.\n",
    "                 If verbose == 2, for each fold, it prints \n",
    "                             the parameters of the first trained algorithm,\n",
    "                             the parameters of the second trained algorithm, and \n",
    "                             the errors of both algorithms as an array with two elements \n",
    "        \n",
    "    Returns:\n",
    "        delta: The estimated difference: the error rate of the first algorithm minus \n",
    "            the error rate of the second algorithm computed using k-fold cross-validation.\n",
    "        std: The sample standard deviation of the estimated difference of errors.            \n",
    "    '''\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(X.shape[0])\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "    errors1 = []\n",
    "    errors2 = []\n",
    "    X_batch_size = X.shape[0]//k\n",
    "    y_batch_size = y.shape[0]//k\n",
    "    for i in range(k):\n",
    "        learn_alg1_copy = copy.deepcopy(learn_alg1)\n",
    "        learn_alg2_copy = copy.deepcopy(learn_alg2)\n",
    "        X_train = np.concatenate((X[:i*X_batch_size], X[(i+1)*X_batch_size:]))\n",
    "        y_train = np.concatenate((y[:i*y_batch_size], y[(i+1)*y_batch_size:]))\n",
    "        X_test = X[i*X_batch_size:(i+1)*X_batch_size]\n",
    "        y_test = y[i*y_batch_size:(i+1)*y_batch_size]\n",
    "        learn_alg1_copy.fit(X_train, y_train)\n",
    "        learn_alg2_copy.fit(X_train, y_train)\n",
    "        errors1.append(1 - learn_alg1_copy.score(X_test, y_test))\n",
    "        errors2.append(1 - learn_alg2_copy.score(X_test, y_test))\n",
    "        if verbose == 2:\n",
    "            print(f'fold {i+1}:')\n",
    "            print(f'  {learn_alg1_copy.par}')\n",
    "            print(f'  {learn_alg2_copy.par}')\n",
    "            print(f'  errors: {errors1[-1]}, {errors2[-1]}')\n",
    "        elif verbose == 1:\n",
    "            print(f'fold {i+1}: {errors1[-1]}, {errors2[-1]}')\n",
    "    delta = np.array(errors1) - np.array(errors2)\n",
    "    std = np.sqrt((1/(k-1))*np.sum((delta - delta.mean())**2))\n",
    "    \n",
    "    return delta.mean(), std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35083cae-f2af-4f97-88d7-380938d722a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "936fd9de4de648bedcc0b4e45ed3d4e4",
     "grade": false,
     "grade_id": "cell-d9ba59175c79303b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the next cell the implementatios of `cross_val` will be tested. The two visibe tests are followed with several hidden tests **(4 points for this part)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23d1a8eb-bd46-4703-8a7e-d7660a904196",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91e883e467b199e9826d4725f0a8f075",
     "grade": true,
     "grade_id": "cell-3a095d9201c8012b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1: 0.15000000000000002, 0.09999999999999998\n",
      "fold 2: 0.22499999999999998, 0.17500000000000004\n",
      "fold 3: 0.09999999999999998, 0.125\n",
      "fold 4: 0.125, 0.125\n",
      "fold 5: 0.275, 0.275\n",
      "Error difference: 0.01499999999999999 std: 0.03354101966249685\n",
      "fold 1: 0.09999999999999998, 0.15000000000000002\n",
      "fold 2: 0.050000000000000044, 0.06666666666666665\n",
      "fold 3: 0.15000000000000002, 0.09999999999999998\n",
      "fold 4: 0.033333333333333326, 0.01666666666666672\n",
      "fold 5: 0.09999999999999998, 0.1333333333333333\n",
      "fold 6: 0.08333333333333337, 0.050000000000000044\n",
      "fold 7: 0.033333333333333326, 0.033333333333333326\n",
      "fold 8: 0.050000000000000044, 0.15000000000000002\n",
      "fold 9: 0.050000000000000044, 0.033333333333333326\n",
      "fold 10: 0.06666666666666665, 0.1166666666666667\n",
      "Error difference: -0.01333333333333333 std: 0.04567734398020992\n"
     ]
    }
   ],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.2, random_state=5)\n",
    "\n",
    "delta, sigma = cross_val(5, Perceptron(init_weights=[1,-2,1], max_epochs=10), \n",
    "             Perceptron(init_weights=[1,-2,1], max_epochs=100), X, y, shuffle=False, verbose=1)\n",
    "print(f\"Error difference: {delta} std: {sigma}\")\n",
    "assert_allclose((delta, sigma), (0.015, 0.03354101967))\n",
    "\n",
    "X, y = make_blobs(n_samples=600, centers=2, n_features=2, random_state=4)\n",
    "\n",
    "delta, sigma = cross_val(10, Perceptron(init_weights=[1,1,1], max_epochs=10), \n",
    "             Perceptron(init_weights=[1,1,1], max_epochs=100), X, y, shuffle=False, verbose=1)\n",
    "print(f\"Error difference: {delta} std: {sigma}\")\n",
    "assert_allclose((delta, sigma), (-0.0133333333, 0.045677344))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249a5d91-603f-4d30-a91e-e6fa6e8a8cbd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0377f93f199b05e3ed93c874af96a9cc",
     "grade": false,
     "grade_id": "cell-d6c5ca081c3a5a82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3: Implement computing of confidence interval (1 point)\n",
    "When applying $k$-fold cross-validation, we will compute the confidence interval to estimate the difference of errors computed by the `cross_val()` function. Implement the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "433aab44-ceb4-4e4d-af4a-401d630b0087",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41f29f87fe3ed1233e1596b674547b40",
     "grade": false,
     "grade_id": "cell-db211ada03beb40c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "def conf_interval(delta: float, sigma: float, conf_level: float, k: int) -> Tuple[float,float]:\n",
    "    \"\"\"Compute confidence interval for the estimated difference of errors d \n",
    "    with standard deviation s returned from cross_val().\n",
    "    \n",
    "        low, high = conf_inteval(delta, sigma, conf_level, k)\n",
    "    \n",
    "    Args:\n",
    "        delta: The difference of errors computed by k-fold cross-validation.\n",
    "        sigma: The standard deviation of the difference of errors computed \n",
    "            by k-fold cross-validation.\n",
    "        conf_level: The confidence level. A value between 0 and 1.\n",
    "        k: The number of folds used in k-fold cross-validation.\n",
    "    \"\"\"\n",
    "    t_value = t.ppf((1 + conf_level)/2, k-1)\n",
    "    low = delta - t_value * sigma / np.sqrt(k)\n",
    "    high = delta + t_value * sigma / np.sqrt(k)\n",
    "    return low, high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40037c9-8eb8-48ff-a5c4-a9a5496d7b4c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9bfd1bddf27713350013a9b03a57c467",
     "grade": false,
     "grade_id": "cell-480f4d171559014f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Function `conf-interval` must pass the following and several hidden tests **(1 point for this part)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f406a4bc-94f4-46fb-8815-ce664d5f0fb5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae1d415de6d1e16a704d2f23fd1c7650",
     "grade": true,
     "grade_id": "cell-bcb06b75fb170e7f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_allclose(conf_interval(0.1, 0.03, 0.9, 10), (0.08260956,0.11739044))\n",
    "assert_allclose(conf_interval(-0.1, 0.03, 0.95, 7), (-0.12774537,-0.07225463))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd7df7-7ce4-4675-90d7-29bc78187708",
   "metadata": {},
   "source": [
    "## Task 4: Compare learning algorithms using $k$-fold cross-validation (3 points)\n",
    "<a id=\"compare_learn_alg\"></a>\n",
    "\n",
    "The above learning algorithms can be compared using the above function `cross_val` on the following datasets `dataset1` and `dataset2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bcf8a6a-9a4d-49d7-92d7-a0d3828b4d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04099436  1.71806508  1.        ]\n",
      "[ 0.96478168 -2.06722166  0.71740289  3.22099464  2.9890881   0.        ]\n"
     ]
    }
   ],
   "source": [
    "dataset1 = np.genfromtxt('Data1.txt', delimiter=' ', dtype = float)\n",
    "print(dataset1[0])\n",
    "dataset2 = np.genfromtxt('Data2.txt', delimiter=' ', dtype = float)\n",
    "print(dataset2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d66618-5f61-4796-ba46-e87e6d5fbf17",
   "metadata": {},
   "source": [
    "### Compare Perceptron with Memorizer (1 point)\n",
    "Compare `Perceptron([1, 1, -1], 1, 20)` and `Memorizer(rng_seed=2023)` on `dataset1` using 5-fold cross-validation with the confidence level `0.95`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b0bfab5-9f63-48a1-b292-c1e2b92955c5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf30094bba15d45343c208b25ff75a2d",
     "grade": false,
     "grade_id": "cell-663ef55413d37e29",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error difference: -0.35 std: 0.12247448713915891\n"
     ]
    }
   ],
   "source": [
    "delta, sigma = cross_val(5,\n",
    "                         Perceptron([1, 1, -1], 1, 20),\n",
    "                         Memorizer(rng_seed=2023),\n",
    "                         dataset1[:,:-1],\n",
    "                         dataset1[:,-1], shuffle=True,\n",
    "                         verbose=0)\n",
    "print(f\"Error difference: {delta} std: {sigma}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3281b5c2-0718-437b-b11c-c94820974480",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b84e9327b81d5e7eab1da9ce074477d4",
     "grade": false,
     "grade_id": "cell-3764ab162aead22b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Fill in the lower and upper limits of the corresponding confidence interval for the difference between the errors of the Perceptron and Memorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1224329-16b8-416f-8b78-9fc86cfb3207",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e18e5ebf08b5f71bec35d152e219e5d",
     "grade": true,
     "grade_id": "cell-b5f2fdb55c2eb2f7",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence interval: [-0.5020721613791638, -0.19792783862083616]\n"
     ]
    }
   ],
   "source": [
    "low, high = conf_interval(delta, sigma, 0.95, 5)\n",
    "print(f\"Confidence interval: [{low}, {high}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b717057a-6334-4ca5-8b07-89659c285f84",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf6287a25b798d6510ead7ec757e9ad2",
     "grade": false,
     "grade_id": "cell-54a588f65f6980dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Is the error difference between the above two learning algorithms statistically significant? Explain your answer! An answer 'YES' or 'NO' will be graded with 0 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b939602b-39c9-4516-841a-4f60b72c713f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b316facdc4a6034f7f2813b7e09c73cc",
     "grade": true,
     "grade_id": "cell-5bf07c46d5cb2c10",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Yes, the difference is statistically significant because the confidence interval does not contains '0' which would be the value where there are no difference between the two algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dba31e-465c-44d9-9abd-c0ad21fb52db",
   "metadata": {},
   "source": [
    "### Compare two perceptrons (1 point)\n",
    "\n",
    "Compare `Perceptron([1, -1, 1, -1, 1, -1], 1, 10)` and `Perceptron([1, -1, 1, -1, 1, -1], 1, 100)` on `dataset2` using 6-fold cross-validation with the confidence level `0.99`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8179bd98-5948-4899-9365-099793895137",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf84afbe5d0e83f2f74b904e79275b29",
     "grade": false,
     "grade_id": "cell-663ef55413d37e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error difference: -0.01833333333333335 std: 0.0842417157153549\n"
     ]
    }
   ],
   "source": [
    "delta, sigma = cross_val(6, \n",
    "                        Perceptron([1, -1, 1, -1, 1, -1], 1, 10),\n",
    "                        Perceptron([1, -1, 1, -1, 1, -1], 1, 100),\n",
    "                        dataset2[:,:-1],\n",
    "                        dataset2[:,-1],\n",
    "                        shuffle=True,\n",
    "                        verbose=0)\n",
    "print(f\"Error difference: {delta} std: {sigma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735ebd52-7ae3-48c4-b719-e1be21f9984c",
   "metadata": {},
   "source": [
    "Fill in the lower and upper limits of the corresponding confidence interval for the difference between the errors of the Perceptron and Memorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9df83828-0be5-49aa-82c0-5390787069f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6802ff3d914526c7e12f891d9676058d",
     "grade": true,
     "grade_id": "cell-eab07e36fea14fa1",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence interval: [-0.15700492562935545, 0.12033825896268875]\n"
     ]
    }
   ],
   "source": [
    "low, high = conf_interval(delta, sigma, 0.99, 6)\n",
    "print(f\"Confidence interval: [{low}, {high}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dd252f-13c4-4aca-a1bf-4f938a6d1a08",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7db669c07d2fb354b01a9e43cae43ba2",
     "grade": false,
     "grade_id": "cell-55c9634523a71a79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Is the error difference between the above two learning algorithms statistically significant? Explain your answer! An answer 'YES' or 'NO' will be graded with 0 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91269f8-6306-4626-8047-cc35716f9e8c",
   "metadata": {},
   "source": [
    "Not at all, the confidence interval contains 0 with a confidence level of 0.99 so this is not statiscally significant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
